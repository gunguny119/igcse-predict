{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxHcJlU7FHFG"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Yz1vtz4U2lS"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4TIZtRmzrZn"
      },
      "outputs": [],
      "source": [
        "sheet_id = \"1kBPSqge0PG6TlHsk46MDq1XD-s2UNeceRxEGqGOTOgo\"\n",
        "sheet_name = 'Sheet1'\n",
        "\n",
        "data_path = f\"https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}\"\n",
        "print(data_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QE3luypFiTKr"
      },
      "outputs": [],
      "source": [
        "topics = [\"1 The particulate nature of matter\",\n",
        "          \"2 Experimental techniques\",\n",
        "          \"3 Atoms, elements and compounds\",\n",
        "          \"4 Stoichiometry\",\n",
        "          \"5 Electricity and chemistry\",\n",
        "          \"6 Chemical energetics\",\n",
        "          \"7 Chemical reactions\",\n",
        "          \"8 Acids, bases and salts\",\n",
        "          \"9 The Periodic Table\",\n",
        "          \"10 Metals\",\n",
        "          \"11 Air and water\",\n",
        "          \"12 Sulfur\",\n",
        "          \"13 Carbonates\",\n",
        "          \"14 Organic chemistry\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vI5GNX97QonY"
      },
      "source": [
        "# Definitions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Y5hL9zxIZkG"
      },
      "source": [
        "## DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZG7XUEKIdm9"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, RobertaTokenizer\n",
        "import pandas as pd\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, df, max_length=128, use_roberta=False):\n",
        "        \"\"\"[\"sent1\", \"sent2\", ....]\"\"\"\n",
        "        topic_mapping = {top: i for i, top in enumerate(topics)}\n",
        "        label = []\n",
        "        for topic in df['topic']:\n",
        "            label.append(topic_mapping[topic])\n",
        "\n",
        "        self.text_list = df['text'].values.tolist()\n",
        "        self.label = label\n",
        "        if use_roberta:\n",
        "            self.tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "        else:\n",
        "            self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        1. get text at index idx from self.text_list\n",
        "        2. tokenizer & encode this text\n",
        "        3. return this along with the label at index idx \n",
        "        \"\"\"\n",
        "        text = self.text_list[idx]\n",
        "        encoded_inputs = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n",
        "        encoded_inputs = {k: v.squeeze(0) for k, v in encoded_inputs.items()}\n",
        "\n",
        "        label = self.label[idx]\n",
        "        return label, encoded_inputs\n",
        "        \n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03SSbcDDKmvC"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkDWf99FKocJ"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from transformers import BertModel, RobertaModel\n",
        "\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self, out_dim, use_roberta=False):\n",
        "        super().__init__()\n",
        "        if use_roberta:\n",
        "            self.bert = RobertaModel.from_pretrained('roberta-base')\n",
        "        else:\n",
        "            self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        self.layer1 = nn.Linear(768, out_dim)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        sentence_emb = output['last_hidden_state'][:, 0, :]  # [batch size, num hidden dim]\n",
        "\n",
        "        return self.layer1(sentence_emb)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDsM9IyuIheo"
      },
      "source": [
        "## Training & Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozXLXAtdIich"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, classification_report\n",
        "\n",
        "def train(model, dataloader, test_loader, loss, lr, num_epochs, save_dir='/content/drive/MyDrive/models', subject='chemistry'):\n",
        "    # pytorch training loop\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    best_score = 0\n",
        "    history = {'accuracy': [], 'auc': []}\n",
        "    for epoch in range(num_epochs):\n",
        "        pbar = tqdm(dataloader)\n",
        "        model.train()\n",
        "        for mini_batch in pbar:  \n",
        "            y, x = mini_batch\n",
        "            x = {k: v.to('cuda') for k, v in x.items()}\n",
        "            y = y.to('cuda')\n",
        "            h = model(x['input_ids'], x['attention_mask'])\n",
        "            j = loss(h, y)\n",
        "            \n",
        "            # do gradient descent\n",
        "            optimizer.zero_grad()  # remove junk from last step\n",
        "            j.backward()   # calculate gradient from current batch outputs\n",
        "            optimizer.step()  # update the weights using the gradients\n",
        "\n",
        "        model.eval()\n",
        "        acc, auc = evaluate(model, test_loader)\n",
        "        history['accuracy'].append(acc)\n",
        "        history['auc'].append(auc)\n",
        "\n",
        "        if acc > best_score:\n",
        "            best_score = acc\n",
        "            torch.save(model.state_dict(), f'{save_dir}/{subject}_best.pth')\n",
        "    \n",
        "    return history\n",
        "\n",
        "@torch.no_grad()\n",
        "def infer(model, loader, threshold=None):\n",
        "    probs = []\n",
        "    label = []\n",
        "    for mini_batch in loader:\n",
        "        y, x = mini_batch\n",
        "        x = {k: v.to('cuda') for k, v in x.items()}\n",
        "        h = torch.softmax(model(x['input_ids'], x['attention_mask']), -1)\n",
        "\n",
        "        if threshold is not None:\n",
        "            idx = h.max(-1)[0] >= threshold\n",
        "            h = h[idx]\n",
        "            y = y[idx]\n",
        "\n",
        "        probs.append(h.cpu().detach().numpy())\n",
        "        label.append(y.numpy())\n",
        "\n",
        "    probs = np.concatenate(probs, 0)\n",
        "    label = np.concatenate(label, 0)\n",
        "    \n",
        "    return probs, label\n",
        "    \n",
        "@torch.no_grad()\n",
        "def evaluate(model, test_loader, threshold=None):\n",
        "    probs, label = infer(model, test_loader, threshold=threshold)\n",
        "    pred = np.argmax(probs, 1)\n",
        "\n",
        "    accuracy = accuracy_score(label, pred)\n",
        "    if threshold is not None:\n",
        "        return accuracy, probs, pred, label\n",
        "\n",
        "    auc = roc_auc_score(label, probs, multi_class='ovr')\n",
        "    print(classification_report(label, pred, target_names=topics))    \n",
        "    return accuracy, auc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpT3b6yqNmj-"
      },
      "source": [
        "# Do Training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FBrvF_3Vb03"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "\n",
        "pmt_df = pd.read_csv('pmt_train.csv').drop_na()\n",
        "\n",
        "if os.path.isfile('all_data.csv'):\n",
        "    manual_df = pd.read_csv('all_data.csv')\n",
        "    manual_df = manual_df[~manual_df['text'].isna() & ~manual_df['topic'].isna()]\n",
        "\n",
        "    df = pd.concat([pmt_df, manual_df])\n",
        "else:\n",
        "    df = pmt_df\n",
        "\n",
        "idx = np.arange(len(df))\n",
        "train_idx, test_idx = train_test_split(idx, test_size=0.1, stratify=df['topic'].values)\n",
        "\n",
        "train_df = df.iloc[train_idx]\n",
        "test_df = df.iloc[test_idx]\n",
        "\n",
        "print(len(train_df), len(test_df))\n",
        "print()\n",
        "train_df['topic'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nubKdCUHO6MF"
      },
      "outputs": [],
      "source": [
        "# hyper parameters\n",
        "lr = 1e-5\n",
        "num_epochs = 10\n",
        "batch_size = 32\n",
        "max_len = 128\n",
        "use_roberta = False\n",
        "\n",
        "# Subject Related\n",
        "out_dim = 14\n",
        "subject='chemistry'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovTZ5FoLNaFA"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "model = MyModel(out_dim, use_roberta=use_roberta).to('cuda')\n",
        "\n",
        "train_dataset = TextDataset(train_df, max_len, use_roberta=use_roberta)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_dataset = TextDataset(test_df, max_len, use_roberta=use_roberta)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)  \n",
        "loss = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "history = train(model, train_loader, test_loader, loss, lr, num_epochs, subject=subject)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NiVxJn_hUhrf"
      },
      "outputs": [],
      "source": [
        "print(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XVKhxrIaB9n"
      },
      "outputs": [],
      "source": [
        "acc, probs, pred, label = evaluate(model, test_loader, threshold=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IiXyZylEi3fG"
      },
      "outputs": [],
      "source": [
        "acc, np.unique(pred), np.unique(label), len(pred), len(test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjZWUq9qR-_O"
      },
      "source": [
        "## Load Saved Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhHP9KsURghm"
      },
      "outputs": [],
      "source": [
        "model = MyModel(out_dim)\n",
        "\n",
        "model.load_state_dict(torch.load(f'/content/drive/MyDrive/models/{subject}_best.pth'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSF5N2h9Xe6L"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "CgKA8TbVj2V_"
      ],
      "name": "BertTrain.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.10 ('tf-m1')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "e79170ae290057d802c0bc8323610d82df653b25ece5c46593757efbea4a04e3"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
